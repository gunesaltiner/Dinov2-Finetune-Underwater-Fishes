{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Image\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from natsort import natsorted\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import wandb\n",
    "import math\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from natsort import natsorted\n",
    "from datasets import Dataset, DatasetDict, Image\n",
    "\n",
    "def get_paths():\n",
    "    train_path_imgs = \"/home/aleximu/gunes/dinov2/project/dataset/fishency/train/fishes\"\n",
    "    train_path_masks = \"/home/aleximu/gunes/dinov2/project/dataset/fishency/train/masks\"\n",
    "    val_path_imgs = \"/home/aleximu/gunes/dinov2/project/dataset/fishency/validation/imgs\"\n",
    "    val_path_masks = \"/home/aleximu/gunes/dinov2/project/dataset/fishency/validation/masks\"\n",
    "    \n",
    "    return train_path_imgs, train_path_masks, val_path_imgs, val_path_masks\n",
    "\n",
    "train_path_imgs, train_path_masks, val_path_imgs, val_path_masks = get_paths()\n",
    "\n",
    "def convert_masks(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        _, black_white_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "        cv2.imwrite(image_path, black_white_image)\n",
    "    \n",
    "convert_masks(train_path_masks)\n",
    "convert_masks(val_path_masks)\n",
    "\n",
    "def get_image_paths(folder_path, limit=None):\n",
    "    image_paths = []\n",
    "    for dirpath, _, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            image_paths.append(os.path.join(dirpath, filename))\n",
    "            if limit and len(image_paths) >= limit:\n",
    "                return natsorted(image_paths)\n",
    "    return natsorted(image_paths)\n",
    "\n",
    "def create_dataset_dict(image_paths, mask_paths):\n",
    "    dataset = Dataset.from_dict({\"image\": image_paths, \"label\": mask_paths})\n",
    "    dataset = dataset.cast_column(\"image\", Image())\n",
    "    dataset = dataset.cast_column(\"label\", Image())\n",
    "    return dataset\n",
    "\n",
    "def initialize_dataset():\n",
    "    train_path_imgs, train_path_masks, val_path_imgs, val_path_masks = get_paths()\n",
    "    image_paths_train = get_image_paths(train_path_imgs, limit=10000)\n",
    "    label_paths_train = get_image_paths(train_path_masks, limit=10000)\n",
    "    image_paths_val = get_image_paths(val_path_imgs)\n",
    "    label_paths_val = get_image_paths(val_path_masks)\n",
    "\n",
    "    train_dataset = create_dataset_dict(image_paths_train, label_paths_train)\n",
    "    val_dataset = create_dataset_dict(image_paths_val, label_paths_val)\n",
    "    dataset = DatasetDict({\"train\": train_dataset, \"validation\": val_dataset})\n",
    "    return dataset\n",
    "\n",
    "def create_id2label():\n",
    "    id2label = {0: 'background', 1: 'fish'}\n",
    "    with open('id2label.json', 'w') as fp:\n",
    "        json.dump(id2label, fp)\n",
    "\n",
    "dataset = initialize_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your example data\n",
    "example = dataset[\"train\"][5]\n",
    "image, segmentation_map = example[\"image\"], example[\"label\"]\n",
    "\n",
    "# Plot both image and segmentation map\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('Image')\n",
    "ax1.axis('off')\n",
    "ax2.imshow(segmentation_map, cmap='viridis')\n",
    "ax2.set_title('Segmentation Map')\n",
    "ax2.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"background\", 1: \"fish\"}\n",
    "print(id2label)\n",
    "segmentation_map = np.array(segmentation_map)\n",
    "print(segmentation_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_map(image, segmentation_map):\n",
    "    # Assuming segmentation_map is (H, W, 3) and each channel has the same values\n",
    "    # Convert it to a 2D array (H, W), this assumes all channels are the same so we use the first one\n",
    "        # Convert segmentation map to 2D if it's 3D, assuming all channels are the same\n",
    "    if segmentation_map.ndim == 3:\n",
    "        segmentation_map_2d = segmentation_map[:, :, 0]\n",
    "    else:\n",
    "        segmentation_map_2d = segmentation_map\n",
    "    \n",
    "    # Find unique labels in the segmentation map\n",
    "    unique_labels = np.unique(segmentation_map_2d)\n",
    "    \n",
    "    # Generate a random color for each label\n",
    "    id2color = {label: list(np.random.choice(range(256), size=3)) for label in unique_labels}\n",
    "\n",
    "    # Initialize an empty color_seg array with the same shape as the original image\n",
    "    color_seg = np.zeros((segmentation_map_2d.shape[0], segmentation_map_2d.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    # Apply colors based on the simplified segmentation map\n",
    "    for label, color in id2color.items():\n",
    "        mask = segmentation_map_2d == label\n",
    "        color_seg[mask] = color\n",
    "\n",
    "    # Blend the original image with the colored segmentation map\n",
    "    img = np.array(image).astype(float) * 0.5 + color_seg.astype(float) * 0.5\n",
    "    img = img.astype(np.uint8)\n",
    "\n",
    "    # Display the result\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Hide axis for better visualization\n",
    "    plt.show()\n",
    "\n",
    "visualize_map(image, segmentation_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "class TwoRandomApply:\n",
    "    def __init__(self,transform_image, transform_label):\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_label = transform_label\n",
    "        self.state = None\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        # Save the current RNG state\n",
    "        state = torch.get_rng_state()\n",
    "        \n",
    "        # Apply the transformation to the image\n",
    "        img = self.transform_image(img)\n",
    "        \n",
    "        # Restore the RNG state to ensure the same randomness for the label\n",
    "        torch.set_rng_state(state)\n",
    "        target = self.transform_label(target)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "  def __init__(self, dataset, transform):\n",
    "    self.dataset = dataset\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.dataset[idx]\n",
    "    original_image = np.array(item[\"image\"])\n",
    "    original_segmentation_map = np.array(item[\"label\"])\n",
    "    \n",
    "    transformed_image, transformed_target = self.transform(original_image, original_segmentation_map)\n",
    "    image = torch.tensor(transformed_image)\n",
    "    target = (torch.tensor(transformed_target)).to(torch.int64)\n",
    "    target = target.view(448, 448)\n",
    "\n",
    "    return image, target, original_image, original_segmentation_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeAndPad:\n",
    "    def __init__(self, target_size, multiple):\n",
    "        self.target_size = target_size\n",
    "        self.multiple = multiple\n",
    "        \n",
    "    def __call__(self, image_array):\n",
    "        image = Image.fromarray(image_array)\n",
    "        # Resize the image\n",
    "        img = transforms.Resize(self.target_size)(image)\n",
    "        \n",
    "        # Calculate padding\n",
    "        pad_width = (self.multiple - img.width % self.multiple) % self.multiple\n",
    "        pad_height = (self.multiple - img.height % self.multiple) % self.multiple\n",
    "\n",
    "        # Apply padding\n",
    "        img = transforms.Pad((pad_width // 2, pad_height // 2, pad_width - pad_width // 2, pad_height - pad_height // 2))(img)\n",
    "        \n",
    "        return img\n",
    "\n",
    "target_size = (448, 448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = np.array([0.485, 0.456, 0.406])\n",
    "STD = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform_image = transforms.Compose([ResizeAndPad(target_size, 14),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomVerticalFlip(),\n",
    "                                      transforms.RandomRotation(360),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=MEAN, std=STD)])\n",
    "\n",
    "train_transform_label = transforms.Compose([ResizeAndPad(target_size, 14),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            transforms.RandomVerticalFlip(),\n",
    "                                            transforms.RandomRotation(360),\n",
    "                                            transforms.Grayscale(num_output_channels=1),\n",
    "                                            transforms.ToTensor()])\n",
    "\n",
    "validation_transform_image = transforms.Compose([ResizeAndPad(target_size, 14),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=MEAN, std=STD)])\n",
    "\n",
    "validation_transform_label = transforms.Compose([ResizeAndPad(target_size, 14),\n",
    "                                                 transforms.Grayscale(num_output_channels=1),\n",
    "                                                 transforms.ToTensor()])\n",
    "\n",
    "train_transformation = TwoRandomApply(train_transform_image, train_transform_label)\n",
    "validation_transformation = TwoRandomApply(validation_transform_image, validation_transform_label)\n",
    "\n",
    "train_dataset = SegmentationDataset(dataset[\"train\"], transform=train_transformation)\n",
    "validation_dataset = SegmentationDataset(dataset[\"validation\"], transform=validation_transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values, target, original_image, original_segmentation_map = train_dataset[0]\n",
    "print(pixel_values.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_transformed_images_and_labels(dataset, index):\n",
    "    # Get the transformed image, target, and their originals from the dataset\n",
    "    pixel_values, target, original_image, original_segmentation_map = dataset[index]\n",
    "    \n",
    "    # Convert the transformed image and label back to NumPy arrays for visualization\n",
    "    transformed_image = pixel_values.permute(1, 2, 0).numpy()\n",
    "    transformed_image = (transformed_image * STD + MEAN)  # Unnormalize\n",
    "    transformed_image = transformed_image.clip(0, 1)  # Clip to ensure it's between 0 and 1\n",
    "    transformed_label = target.numpy()\n",
    "\n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "    axs[0, 0].imshow(original_image)\n",
    "    axs[0, 0].set_title('Original Image')\n",
    "    axs[0, 0].axis('off')\n",
    "\n",
    "    axs[0, 1].imshow(original_segmentation_map, cmap='gray')\n",
    "    axs[0, 1].set_title('Original Label')\n",
    "    axs[0, 1].axis('off')\n",
    "\n",
    "    axs[1, 0].imshow(transformed_image)\n",
    "    axs[1, 0].set_title('Transformed Image')\n",
    "    axs[1, 0].axis('off')\n",
    "\n",
    "    axs[1, 1].imshow(transformed_label, cmap='gray')\n",
    "    axs[1, 1].set_title('Transformed Label')\n",
    "    axs[1, 1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'train_dataset' is your dataset instance and you want to inspect the first item\n",
    "show_transformed_images_and_labels(train_dataset, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(inputs):\n",
    "    batch = dict()\n",
    "    batch[\"pixel_values\"] = torch.stack([i[0] for i in inputs], dim=0)\n",
    "    batch[\"labels\"] = torch.stack([i[1] for i in inputs], dim=0)\n",
    "    batch[\"original_images\"] = [i[2] for i in inputs]\n",
    "    batch[\"original_segmentation_maps\"] = [i[3] for i in inputs]\n",
    "\n",
    "    return batch\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v,torch.Tensor):\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"pixel_values\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"labels\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_image = (batch[\"pixel_values\"][5].numpy() * np.array(STD)[:, None, None]) + np.array(MEAN)[:, None, None]\n",
    "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "unnormalized_image = Image.fromarray(unnormalized_image)\n",
    "unnormalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[id2label[id] for id in torch.unique(batch[\"labels\"][5]).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2d = batch[\"labels\"][5].numpy()\n",
    "\n",
    "visualize_map(unnormalized_image, label_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Dinov2Model, Dinov2PreTrainedModel\n",
    "from transformers.modeling_outputs import SemanticSegmenterOutput\n",
    "\n",
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_labels=2):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "\n",
    "        #This conv2D layer converts the patch embeddings into a logits tensor of shape (batch_size, num_labels, height, width)\n",
    "        # in_channels ==> embedding_dimension\n",
    "        self.classifier = torch.nn.Conv2d(in_channels, num_labels, (1,1))\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels) #(batch_size, 32, 32, 768)\n",
    "        embeddings = embeddings.permute(0,3,1,2)  # (batch_size, 768, 32, 32)\n",
    "\n",
    "        return self.classifier(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_labels=2):\n",
    "        super(DeepLinearClassifier, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "                                            torch.nn.Conv2d(in_channels, 128, (1,1)),\n",
    "                                            torch.nn.ReLU(), #to add non-linearity\n",
    "                                            torch.nn.Conv2d(128, 64, (1,1)),\n",
    "                                            torch.nn.ReLU(),\n",
    "                                            torch.nn.Conv2d(64, 32, (1,1)),\n",
    "                                            torch.nn.ReLU(),\n",
    "                                            torch.nn.Conv2d(32, num_labels, (1,1))\n",
    "                                            )\n",
    "\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels) #(batch_size, 32, 32, 768)\n",
    "        embeddings = embeddings.permute(0,3,1,2)  # (batch_size, 768, 32, 32)\n",
    "\n",
    "        return self.classifier(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Dinov2ForSemanticSegmentation(Dinov2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.dinov2 = Dinov2Model(config)\n",
    "        \n",
    "        \n",
    "        #takes patch embeddings from an image based on the features extracted by DINO /// predicts a label for each patch\n",
    "        #self.classifier = LinearClassifier(config.hidden_size, 32, 32, config.num_labels) \n",
    "                                                                                          \n",
    "        self.classifier = DeepLinearClassifier(config.hidden_size, 32, 32, config.num_labels) \n",
    "\n",
    "    def forward(self, pixel_values, output_hidden_states=True, output_attentions=True, labels=None):\n",
    "        \n",
    "        #Frozen features are used\n",
    "        #pixel_values are input ! dinov2 model is applied to the input to extract visual features\n",
    "        \n",
    "        #print(self.dinov2)\n",
    "        \n",
    "        outputs = self.dinov2(pixel_values, output_hidden_states=output_hidden_states, output_attentions=output_attentions)\n",
    "\n",
    "        # get the patch embeddings to exclude the CLS token\n",
    "        #CLS token includes summerized informarmation (global features) but we need to local features to segmentation (pixel)\n",
    "        #[batch_size, sequence_length, embedding_dimension] ==> sequence_length includes the embeddings for all patches and the CLS token\n",
    "        print(outputs.last_hidden_state.shape)\n",
    "        patch_embeddings = outputs.last_hidden_state[:, 1:, :]\n",
    "        print(patch_embeddings.shape)\n",
    "\n",
    "        # convert to logits and upsample to the size of the pixel values\n",
    "        logits = self.classifier(patch_embeddings)\n",
    "\n",
    "        # Interpolation refers to the method used to estimate the values at new points based on the values at known points\n",
    "        # resizing logits according to height and weight of target image to calculate loss\n",
    "        #print(\"first logits shape: \", logits.shape)\n",
    "        logits = F.interpolate(logits, size=pixel_values.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        #print(\"second logits shape: \", logits.shape)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.squeeze()\n",
    "            print(\"labels shape: \", labels.shape)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        return SemanticSegmenterOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dinov2ForSemanticSegmentation.from_pretrained(\"facebook/dinov2-base\", id2label=id2label, num_labels=len(id2label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"dinov2\"):\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(pixel_values=batch[\"pixel_values\"], labels=batch[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def pixel_accuracy_for_fish(preds, labels, fish_class=1):\n",
    "    fish_mask = (labels == fish_class)\n",
    "    correct = (preds[fish_mask] == fish_class).sum().item()\n",
    "    total = fish_mask.sum().item()\n",
    "    return correct / total if total != 0 else float('nan')\n",
    "\n",
    "\n",
    "\n",
    "def manual_iou(preds, labels, num_classes):\n",
    "    iou_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (preds == cls)\n",
    "        target_inds = (labels == cls)\n",
    "        intersection = (pred_inds & target_inds).sum()\n",
    "        union = pred_inds.sum() + target_inds.sum() - intersection\n",
    "        if union == 0:\n",
    "            iou = float('nan')  # avoid division by zero\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "        iou_per_class.append(iou if not math.isnan(iou) else float('nan'))\n",
    "    valid_iou = [iou for iou in iou_per_class if not math.isnan(iou)]\n",
    "    mean_iou = sum(valid_iou) / len(valid_iou) if valid_iou else float('nan')\n",
    "    return mean_iou, iou_per_class\n",
    "\n",
    "def pixel_accuracy(preds, labels):\n",
    "    correct = (preds == labels).sum().item()\n",
    "    total = labels.numel()\n",
    "    return correct / total\n",
    "\n",
    "def mean_pixel_accuracy(preds, labels, num_classes):\n",
    "    accuracies = []\n",
    "    for cls in range(num_classes):\n",
    "        cls_mask = (labels == cls)\n",
    "        cls_total = cls_mask.sum().item()\n",
    "        if cls_total == 0:\n",
    "            accuracies.append(float('nan'))\n",
    "        else:\n",
    "            correct = (preds[cls_mask] == cls).sum().item()\n",
    "            accuracies.append(correct / cls_total)\n",
    "    valid_accuracies = [acc for acc in accuracies if not math.isnan(acc)]\n",
    "    mean_accuracy = sum(valid_accuracies) / len(valid_accuracies) if valid_accuracies else float('nan')\n",
    "    return mean_accuracy, accuracies\n",
    "\n",
    "\n",
    "def precision_recall_f1(preds, labels, fish_class=1):\n",
    "    pred_inds = (preds == fish_class)\n",
    "    target_inds = (labels == fish_class)\n",
    "    \n",
    "    true_positive = (pred_inds & target_inds).sum().item()\n",
    "    false_positive = (pred_inds & ~target_inds).sum().item()\n",
    "    false_negative = (~pred_inds & target_inds).sum().item()\n",
    "    \n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) != 0 else 0.0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) != 0 else 0.0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        f1_score = 0.0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def evaluate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    eval_steps = 0\n",
    "    total_mean_iou = 0.0\n",
    "    total_pixel_accuracy = 0.0\n",
    "    total_mean_pixel_accuracy = 0.0\n",
    "    total_pixel_accuracy_fish = 0.0\n",
    "    total_precision_fish = 0.0\n",
    "    total_recall_fish = 0.0\n",
    "    total_f1_fish = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            if torch.cuda.device_count() > 1:\n",
    "                loss = loss.mean() \n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            eval_steps += 1\n",
    "\n",
    "            # Convert to probabilities and predictions for IoU and PA\n",
    "            logits = outputs\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            # Calculate IoU\n",
    "            mean_iou, iou_per_class = manual_iou(preds, labels, num_classes)\n",
    "            total_mean_iou += mean_iou\n",
    "\n",
    "            # Calculate Pixel Accuracy (PA)\n",
    "            pa = pixel_accuracy(preds, labels)\n",
    "            total_pixel_accuracy += pa\n",
    "\n",
    "            # Calculate Mean Pixel Accuracy (mPA)\n",
    "            mean_pa, accuracies_per_class = mean_pixel_accuracy(preds, labels, num_classes)\n",
    "            total_mean_pixel_accuracy += mean_pa\n",
    "\n",
    "            # Calculate Pixel Accuracy for Fish\n",
    "            pa_fish = pixel_accuracy_for_fish(preds, labels, fish_class=1)\n",
    "            total_pixel_accuracy_fish += pa_fish\n",
    "\n",
    "            # Calculate Precision, Recall, F1 Score for Fish\n",
    "            precision_fish, recall_fish, f1_fish = precision_recall_f1(preds, labels, fish_class=1)\n",
    "            total_precision_fish += precision_fish\n",
    "            total_recall_fish += recall_fish\n",
    "            total_f1_fish += f1_fish\n",
    "\n",
    "    avg_eval_loss = eval_loss / eval_steps\n",
    "    avg_mean_iou = total_mean_iou / eval_steps\n",
    "    avg_pixel_accuracy = total_pixel_accuracy / eval_steps\n",
    "    avg_mean_pixel_accuracy = total_mean_pixel_accuracy / eval_steps\n",
    "    avg_pixel_accuracy_fish = total_pixel_accuracy_fish / eval_steps\n",
    "    avg_precision_fish = total_precision_fish / eval_steps\n",
    "    avg_recall_fish = total_recall_fish / eval_steps\n",
    "    avg_f1_fish = total_f1_fish / eval_steps\n",
    "\n",
    "    return avg_eval_loss, avg_mean_iou, iou_per_class, avg_pixel_accuracy, avg_mean_pixel_accuracy, avg_pixel_accuracy_fish, avg_precision_fish, avg_recall_fish, avg_f1_fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"DINOv2-FineTuning\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"Architecture\": \"DINOv2-Supervised-Deep-Layer\",\n",
    "    \"Dataset\": \"Fishency\",\n",
    "    \"output_hidden_states\": True,\n",
    "    \"Batch Size\": 64,\n",
    "    \"Learning_Rate\": 0.000062,\n",
    "    \"Scheduler\": \"StepLR(step_size=10, gamma=0.1)\",\n",
    "    \"Epochs\": 50,\n",
    "    \"Optimizer\": torch.optim.SGD,\n",
    "    }\n",
    ")\n",
    "learning_rate = 0.000062\n",
    "epochs = 50\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=4, verbose=True, threshold=1e-4, min_lr=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using \", torch.cuda.device_count(), \"GPUs !!!\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    print(\"Epoch:\", epoch)\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            loss = loss.mean() \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "    # Evaluation step\n",
    "    num_classes = len(id2label)\n",
    "    avg_eval_loss, avg_mean_iou, iou_per_class = evaluate_model(model, validation_dataloader, device, num_classes)\n",
    "    iou_for_fish = iou_per_class[1]\n",
    "\n",
    "    scheduler.step()\n",
    "    wandb.log({\"learning_rate\": scheduler.get_last_lr()[0]})\n",
    "\n",
    "    wandb.log({\n",
    "                \"Validation Loss\": avg_eval_loss,\n",
    "               \"Average Mean IoU\": avg_mean_iou,\n",
    "               \"IoU scores for Fish\": iou_for_fish\n",
    "    })\n",
    "\n",
    "    print(f\"Validation Loss: {avg_eval_loss*100:.2f}%\")\n",
    "    print(f\"Average Mean IoU: {avg_mean_iou*100:.2f}%\")\n",
    "    print(f\"IoU scores for fish: {iou_for_fish*100:.2f}%\")\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "random_index = random.randint(0, len(validation_dataset) - 1)\n",
    "sample = validation_dataset[random_index]\n",
    "\n",
    "with torch.no_grad():\n",
    "    pixel_values, true_mask, original_image, original_segmentation_map = sample\n",
    "    pixel_values = pixel_values.unsqueeze(0).to(device)  # Add batch dimension and send to device\n",
    "    \n",
    "    outputs = model(pixel_values)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    predicted_mask = torch.argmax(probs, dim=1).squeeze().cpu().numpy()  # Remove batch dim\n",
    "\n",
    "# Convert to displayable format\n",
    "original_image_display = np.array(original_image).astype(np.uint8)\n",
    "true_mask_display = np.array(original_segmentation_map).astype(np.uint8)\n",
    "predicted_mask_image = Image.fromarray(predicted_mask.astype(np.uint8))\n",
    "predicted_mask_resized = predicted_mask_image.resize(original_image_display.shape[1::-1], Image.NEAREST)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 10))\n",
    "axes[0].imshow(original_image_display)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(true_mask_display, cmap='jet')\n",
    "axes[1].set_title('True Mask')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(predicted_mask_resized, cmap='jet')\n",
    "axes[2].set_title('Predicted Mask')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# random_index = random.randint(0, len(validation_dataset) - 1)\n",
    "# sample = validation_dataset[random_index]\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     pixel_values, true_mask, original_image, original_segmentation_map = sample\n",
    "#     pixel_values = pixel_values.unsqueeze(0).to(device)  # Add batch dimension and send to device\n",
    "    \n",
    "#     outputs = model(pixel_values)\n",
    "#     logits = outputs.logits\n",
    "#     probs = torch.softmax(logits, dim=1)\n",
    "#     predicted_mask = torch.argmax(probs, dim=1).squeeze().cpu().numpy()  # Remove batch dim\n",
    "\n",
    "# # Convert to displayable format\n",
    "# original_image_display = np.array(original_image).astype(np.uint8)\n",
    "# true_mask_display = np.array(original_segmentation_map).astype(np.uint8)\n",
    "# predicted_mask_image = Image.fromarray(predicted_mask.astype(np.uint8))\n",
    "# predicted_mask_resized = predicted_mask_image.resize(original_image_display.shape[1::-1], Image.NEAREST)\n",
    "\n",
    "\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(20, 10))\n",
    "# axes[0].imshow(original_image_display)\n",
    "# axes[0].set_title('Original Image')\n",
    "# axes[0].axis('off')\n",
    "\n",
    "# axes[1].imshow(true_mask_display, cmap='jet')\n",
    "# axes[1].set_title('True Mask')\n",
    "# axes[1].axis('off')\n",
    "\n",
    "# axes[2].imshow(predicted_mask_resized, cmap='jet')\n",
    "# axes[2].set_title('Predicted Mask')\n",
    "# axes[2].axis('off')\n",
    "\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
