{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Image\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import math\n",
    "from natsort import natsorted\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import wandb\n",
    "import sys\n",
    "sys.path.append(\"/home/aleximu/gunes/dinov2\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "from dinov2.models.vision_transformer import vit_small, vit_base, vit_large, vit_giant2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from natsort import natsorted\n",
    "from datasets import Dataset, DatasetDict, Image\n",
    "import json\n",
    "\n",
    "def get_paths():\n",
    "    train_path_imgs = \"/home/aleximu/gunes/dinov2/project/dataset/fishency/train/fishes\"\n",
    "    train_path_masks = \"/home/aleximu/gunes/dinov2/project/dataset/fishency/train/masks\"\n",
    "    val_path_imgs = \"/home/aleximu/gunes/dinov2/project/dataset/fishency/validation/imgs\"\n",
    "    val_path_masks = \"/home/aleximu/gunes/dinov2/project/dataset/fishency/validation/masks\"\n",
    "\n",
    "    frames_path = \"/home/aleximu/gunes/dinov2/outputs_frames_transformed\"\n",
    "    \n",
    "    return train_path_imgs, train_path_masks, val_path_imgs, val_path_masks, frames_path\n",
    "\n",
    "train_path_imgs, train_path_masks, val_path_imgs, val_path_masks, frames_path = get_paths()\n",
    "\n",
    "def convert_masks(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        # Load the image in grayscale\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Check if the image was loaded correctly\n",
    "        if image is None:\n",
    "            print(f\"Warning: Image at path {image_path} could not be loaded.\")\n",
    "            continue\n",
    "        \n",
    "        # Convert to binary image\n",
    "        _, black_white_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # Save the black and white image\n",
    "        if not cv2.imwrite(image_path, black_white_image):\n",
    "            print(f\"Error: Could not write image to path {image_path}\")\n",
    "\n",
    "    \n",
    "convert_masks(train_path_masks)\n",
    "convert_masks(val_path_masks)\n",
    "\n",
    "def get_image_paths(folder_path):\n",
    "    image_paths = []\n",
    "    for dirpath, _, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            image_paths.append(os.path.join(dirpath, filename))\n",
    "    return natsorted(image_paths)\n",
    "\n",
    "def create_dataset_dict(image_paths, mask_paths):\n",
    "    dataset = Dataset.from_dict({\"image\": image_paths, \"label\": mask_paths})\n",
    "    dataset = dataset.cast_column(\"image\", Image())\n",
    "    dataset = dataset.cast_column(\"label\", Image())\n",
    "    return dataset\n",
    "\n",
    "def initialize_dataset():\n",
    "    train_path_imgs, train_path_masks, val_path_imgs, val_path_masks, frames_path = get_paths()\n",
    "    image_paths_train = get_image_paths(train_path_imgs)\n",
    "    label_paths_train = get_image_paths(train_path_masks)\n",
    "    image_paths_val = get_image_paths(val_path_imgs)\n",
    "    label_paths_val = get_image_paths(val_path_masks)\n",
    "\n",
    "    image_frames = get_image_paths(frames_path)\n",
    "    frame_dataset = create_dataset_dict(image_frames, image_frames)\n",
    "\n",
    "    \n",
    "    video_dataset = DatasetDict({\"image\": frame_dataset})\n",
    "\n",
    "    train_dataset = create_dataset_dict(image_paths_train, label_paths_train)\n",
    "    val_dataset = create_dataset_dict(image_paths_val, label_paths_val)\n",
    "    dataset = DatasetDict({\"train\": train_dataset, \"validation\": val_dataset})\n",
    "    return dataset, video_dataset\n",
    "\n",
    "def create_id2label():\n",
    "    id2label = {0: 'background', 1: 'fish'}\n",
    "    with open('id2label.json', 'w') as fp:\n",
    "        json.dump(id2label, fp)\n",
    "\n",
    "#create_id2label()\n",
    "dataset, video_dataset = initialize_dataset()\n",
    "id2label = {0: \"background\", 1: \"fish\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset\n",
    "video_dataset = video_dataset[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_count = 1000\n",
    "\n",
    "# Randomly select indices for labeled data\n",
    "labeled_indices = random.sample(range(len(dataset['validation'])), labeled_data_count)\n",
    "\n",
    "# Remains are unlabeled data\n",
    "validation_indices = [i for i in range(len(dataset['validation'])) if i not in labeled_indices]\n",
    "\n",
    "labeled_dataset = dataset['validation'].select(labeled_indices)\n",
    "\n",
    "validation_dataset = dataset['validation'].select(validation_indices)\n",
    "\n",
    "unlabeled_dataset = dataset['train']\n",
    "validation_dataset\n",
    "labeled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "class TwoRandomApply:\n",
    "    def __init__(self,transform_image, transform_label):\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_label = transform_label\n",
    "        self.state = None\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        # Save the current RNG state\n",
    "        state = torch.get_rng_state()\n",
    "        \n",
    "        # Apply the transformation to the image\n",
    "        image = self.transform_image(image)\n",
    "        \n",
    "        # Restore the RNG state to ensure the same randomness for the label\n",
    "        torch.set_rng_state(state)\n",
    "        target = self.transform_label(target)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "  def __init__(self, dataset, transform):\n",
    "    self.dataset = dataset\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.dataset[idx]\n",
    "    original_image = np.array(item[\"image\"])\n",
    "\n",
    "    if 'label' in item:\n",
    "      original_segmentation_map = np.array(item[\"label\"])\n",
    "      transformed_image, transformed_target = self.transform(original_image, original_segmentation_map)\n",
    "      image = torch.tensor(transformed_image)\n",
    "      target = (torch.tensor(transformed_target)).to(torch.int64)\n",
    "      target = target.view(448, 448)\n",
    "\n",
    "      return image, target, original_image, original_segmentation_map\n",
    "    \n",
    "    else:\n",
    "      transformed_image = self.transform(original_image)\n",
    "      image = torch.tensor(transformed_image)\n",
    "      \n",
    "      return image, original_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeAndPad:\n",
    "    def __init__(self, target_size, multiple):\n",
    "        self.target_size = target_size\n",
    "        self.multiple = multiple\n",
    "        \n",
    "    def __call__(self, image_array):\n",
    "        image = Image.fromarray(image_array)\n",
    "        # Resize the image\n",
    "        img = transforms.Resize(self.target_size)(image)\n",
    "        \n",
    "        # Calculate padding\n",
    "        pad_width = (self.multiple - img.width % self.multiple) % self.multiple\n",
    "        pad_height = (self.multiple - img.height % self.multiple) % self.multiple\n",
    "\n",
    "        # Apply padding\n",
    "        img = transforms.Pad((pad_width // 2, pad_height // 2, pad_width - pad_width // 2, pad_height - pad_height // 2))(img)\n",
    "        \n",
    "        return img\n",
    "\n",
    "target_size = (448, 448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = np.array([0.485, 0.456, 0.406])\n",
    "STD = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform_image = transforms.Compose([ResizeAndPad(target_size, 14),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomVerticalFlip(),\n",
    "                                      transforms.RandomRotation(360),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=MEAN, std=STD)])\n",
    "\n",
    "train_transform_label = transforms.Compose([ResizeAndPad(target_size, 14),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            transforms.RandomVerticalFlip(),\n",
    "                                            transforms.RandomRotation(360),\n",
    "                                            transforms.Grayscale(num_output_channels=1),\n",
    "                                            transforms.ToTensor()])\n",
    "\n",
    "validation_transform_image = transforms.Compose([ResizeAndPad(target_size, 14),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=MEAN, std=STD)])\n",
    "\n",
    "validation_transform_label = transforms.Compose([ResizeAndPad(target_size, 14),\n",
    "                                                 transforms.Grayscale(num_output_channels=1),\n",
    "                                                 transforms.ToTensor()])\n",
    "\n",
    "labeled_train_transformation = TwoRandomApply(train_transform_image, train_transform_label)\n",
    "validation_transformation = TwoRandomApply(validation_transform_image, validation_transform_label)\n",
    "\n",
    "\n",
    "labeled_train_dataset = SegmentationDataset(labeled_dataset, transform=labeled_train_transformation)\n",
    "validation_dataset = SegmentationDataset(validation_dataset, transform=validation_transformation)\n",
    "unlabeled_train_dataset = SegmentationDataset(unlabeled_dataset, transform=train_transform_image)\n",
    "\n",
    "\n",
    "video_frames = SegmentationDataset(video_dataset, transform=validation_transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values, target, original_image, original_segmentation_map = labeled_train_dataset[0]\n",
    "print(pixel_values.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeled_collate_fn(inputs):\n",
    "    batch = dict()\n",
    "    batch[\"pixel_values\"] = torch.stack([i[0] for i in inputs], dim=0)\n",
    "    batch[\"labels\"] = torch.stack([i[1] for i in inputs], dim=0)\n",
    "    batch[\"original_images\"] = [i[2] for i in inputs]\n",
    "    batch[\"original_segmentation_maps\"] = [i[3] for i in inputs]\n",
    "\n",
    "    return batch\n",
    "\n",
    "def unlabeled_collate_fn(inputs):\n",
    "    batch = dict()\n",
    "    batch[\"pixel_values\"] = torch.stack( [i[0] for i in inputs], dim=0)\n",
    "    batch[\"original_images\"] = [i[1] for i in inputs]\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "labeled_train_dataloader = DataLoader(labeled_train_dataset, batch_size=16, shuffle=True, collate_fn=labeled_collate_fn)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=16, shuffle=False, collate_fn=labeled_collate_fn)\n",
    "unlabeled_train_dataloader = DataLoader(unlabeled_train_dataset, batch_size=16, shuffle=True, collate_fn=unlabeled_collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "video_dataloader = DataLoader(video_frames, batch_size=16, shuffle=False, collate_fn=labeled_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "# Define a new classifier layer that contains a few linear layers with a ReLU to make predictions positive\n",
    "class DinoVisionTransformerSegmentation(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_size=\"base\", num_labels=2):\n",
    "        super(DinoVisionTransformerSegmentation, self).__init__()\n",
    "        self.model_size = model_size\n",
    "        \n",
    "        # loading a model with registers\n",
    "        n_register_tokens = 4\n",
    "        \n",
    "        if model_size == \"small\":        \n",
    "            model = vit_small(patch_size=14,\n",
    "                              img_size=526,\n",
    "                              init_values=1.0,\n",
    "                              num_register_tokens=n_register_tokens,\n",
    "                              block_chunks=0)\n",
    "            self.embedding_size = 384\n",
    "            self.number_of_heads = 6\n",
    "            \n",
    "        elif model_size == \"base\":\n",
    "            model = vit_base(patch_size=14,\n",
    "                             img_size=526,\n",
    "                             init_values=1.0,\n",
    "                             num_register_tokens=n_register_tokens,\n",
    "                             block_chunks=0)\n",
    "            self.embedding_size = 768\n",
    "            self.number_of_heads = 12\n",
    "\n",
    "        elif model_size == \"large\":\n",
    "            model = vit_large(patch_size=14,\n",
    "                              img_size=526,\n",
    "                              init_values=1.0,\n",
    "                              num_register_tokens=n_register_tokens,\n",
    "                              block_chunks=0)\n",
    "            self.embedding_size = 1024\n",
    "            self.number_of_heads = 16\n",
    "            \n",
    "        elif model_size == \"giant\":\n",
    "            model = vit_giant2(patch_size=14,\n",
    "                               img_size=526,\n",
    "                               init_values=1.0,\n",
    "                               num_register_tokens=n_register_tokens,\n",
    "                               block_chunks=0)\n",
    "            self.embedding_size = 1536\n",
    "            self.number_of_heads = 24\n",
    "\n",
    "        self.transformer = model\n",
    "\n",
    "        # self.segmentation_head = torch.nn.Sequential(\n",
    "        #                                     torch.nn.Conv2d(self.embedding_size, 128, (1,1)),\n",
    "        #                                     torch.nn.ReLU(), #to add non-linearity\n",
    "        #                                     torch.nn.Conv2d(128, 64, (1,1)),\n",
    "        #                                     torch.nn.ReLU(),\n",
    "        #                                     torch.nn.Conv2d(64, 32, (1,1)),\n",
    "        #                                     torch.nn.ReLU(),\n",
    "        #                                     torch.nn.Conv2d(32, num_labels, (1,1))\n",
    "        #                                     )\n",
    "         \n",
    "        \n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.Conv2d(self.embedding_size, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, num_labels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        transformer_output = self.transformer.forward_features(pixel_values)\n",
    "        \n",
    "        patch_embeddings = transformer_output[\"x_norm_patchtokens\"]\n",
    "        batch_size = patch_embeddings.size(0)\n",
    "        sequence_length = patch_embeddings.size(1)\n",
    "        embedding_size = patch_embeddings.size(2)\n",
    "        \n",
    "        # Reshape to make it compatible with Conv2d\n",
    "        patch_size = int(math.sqrt(sequence_length))\n",
    "        patch_embeddings = patch_embeddings.permute(0, 2, 1).contiguous().view(batch_size, embedding_size, patch_size, patch_size)\n",
    "        head_output = self.segmentation_head(patch_embeddings)\n",
    "\n",
    "        segmentation_output = F.interpolate(head_output, size=pixel_values.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        return segmentation_output\n",
    "    \n",
    "    def get_last_self_attention(self, pixel_values):\n",
    "        return self.transformer.get_last_self_attention(pixel_values)\n",
    "    \n",
    "\n",
    "# Load the model and state dictionary\n",
    "model = DinoVisionTransformerSegmentation(\"base\")\n",
    "pretrained_path = \"/home/aleximu/gunes/dinov2/dinov2/train/model_0025999.rank_0.pth\"\n",
    "state_dict_trained = torch.load(pretrained_path)\n",
    "\n",
    "# Extract the model state_dict\n",
    "model_state_dict = state_dict_trained['model']\n",
    "\n",
    "# Load the state dictionary into the model, ignoring mismatched keys\n",
    "model_keys = set(model.state_dict().keys())\n",
    "trained_keys = set(model_state_dict.keys())\n",
    "\n",
    "matching_keys = model_keys.intersection(trained_keys)\n",
    "model_state_dict_filtered = {k: v for k, v in model_state_dict.items() if k in matching_keys}\n",
    "\n",
    "missing_keys, unexpected_keys = model.load_state_dict(model_state_dict_filtered, strict=False)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.transformer.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy_for_fish(preds, labels, fish_class=1):\n",
    "    fish_mask = (labels == fish_class)\n",
    "    correct = (preds[fish_mask] == fish_class).sum().item()\n",
    "    total = fish_mask.sum().item()\n",
    "    return correct / total if total != 0 else float('nan')\n",
    "\n",
    "def manual_iou(preds, labels, num_classes):\n",
    "    iou_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (preds == cls)\n",
    "        target_inds = (labels == cls)\n",
    "        intersection = (pred_inds & target_inds).sum()\n",
    "        union = pred_inds.sum() + target_inds.sum() - intersection\n",
    "        if union == 0:\n",
    "            iou = float('nan')  # avoid division by zero\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "        iou_per_class.append(iou if not math.isnan(iou) else float('nan'))\n",
    "    valid_iou = [iou for iou in iou_per_class if not math.isnan(iou)]\n",
    "    mean_iou = sum(valid_iou) / len(valid_iou) if valid_iou else float('nan')\n",
    "    return mean_iou, iou_per_class\n",
    "\n",
    "def pixel_accuracy(preds, labels):\n",
    "    correct = (preds == labels).sum().item()\n",
    "    total = labels.numel()\n",
    "    return correct / total\n",
    "\n",
    "def mean_pixel_accuracy(preds, labels, num_classes):\n",
    "    accuracies = []\n",
    "    for cls in range(num_classes):\n",
    "        cls_mask = (labels == cls)\n",
    "        cls_total = cls_mask.sum().item()\n",
    "        if cls_total == 0:\n",
    "            accuracies.append(float('nan'))\n",
    "        else:\n",
    "            correct = (preds[cls_mask] == cls).sum().item()\n",
    "            accuracies.append(correct / cls_total)\n",
    "    valid_accuracies = [acc for acc in accuracies if not math.isnan(acc)]\n",
    "    mean_accuracy = sum(valid_accuracies) / len(valid_accuracies) if valid_accuracies else float('nan')\n",
    "    return mean_accuracy, accuracies\n",
    "\n",
    "\n",
    "def precision_recall_f1(preds, labels, fish_class=1):\n",
    "    pred_inds = (preds == fish_class)\n",
    "    target_inds = (labels == fish_class)\n",
    "    \n",
    "    true_positive = (pred_inds & target_inds).sum().item()\n",
    "    false_positive = (pred_inds & ~target_inds).sum().item()\n",
    "    false_negative = (~pred_inds & target_inds).sum().item()\n",
    "    \n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) != 0 else 0.0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) != 0 else 0.0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        f1_score = 0.0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def evaluate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    eval_steps = 0\n",
    "    total_mean_iou = 0.0\n",
    "    total_pixel_accuracy = 0.0\n",
    "    total_mean_pixel_accuracy = 0.0\n",
    "    total_pixel_accuracy_fish = 0.0\n",
    "    total_precision_fish = 0.0\n",
    "    total_recall_fish = 0.0\n",
    "    total_f1_fish = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            labels = labels.squeeze()\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            if torch.cuda.device_count() > 1:\n",
    "                loss = loss.mean() \n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            eval_steps += 1\n",
    "\n",
    "            # Convert to probabilities and predictions for IoU and PA\n",
    "            logits = outputs\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            # Calculate IoU\n",
    "            mean_iou, iou_per_class = manual_iou(preds, labels, num_classes)\n",
    "            total_mean_iou += mean_iou\n",
    "\n",
    "            # Calculate Pixel Accuracy (PA)\n",
    "            pa = pixel_accuracy(preds, labels)\n",
    "            total_pixel_accuracy += pa\n",
    "\n",
    "            # Calculate Mean Pixel Accuracy (mPA)\n",
    "            mean_pa, accuracies_per_class = mean_pixel_accuracy(preds, labels, num_classes)\n",
    "            total_mean_pixel_accuracy += mean_pa\n",
    "\n",
    "            # Calculate Pixel Accuracy for Fish\n",
    "            pa_fish = pixel_accuracy_for_fish(preds, labels, fish_class=1)\n",
    "            total_pixel_accuracy_fish += pa_fish\n",
    "\n",
    "            # Calculate Precision, Recall, F1 Score for Fish\n",
    "            precision_fish, recall_fish, f1_fish = precision_recall_f1(preds, labels, fish_class=1)\n",
    "            total_precision_fish += precision_fish\n",
    "            total_recall_fish += recall_fish\n",
    "            total_f1_fish += f1_fish\n",
    "\n",
    "    avg_eval_loss = eval_loss / eval_steps\n",
    "    avg_mean_iou = total_mean_iou / eval_steps\n",
    "    avg_pixel_accuracy = total_pixel_accuracy / eval_steps\n",
    "    avg_mean_pixel_accuracy = total_mean_pixel_accuracy / eval_steps\n",
    "    avg_pixel_accuracy_fish = total_pixel_accuracy_fish / eval_steps\n",
    "    avg_precision_fish = total_precision_fish / eval_steps\n",
    "    avg_recall_fish = total_recall_fish / eval_steps\n",
    "    avg_f1_fish = total_f1_fish / eval_steps\n",
    "\n",
    "    return avg_eval_loss, avg_mean_iou, iou_per_class, avg_pixel_accuracy, avg_mean_pixel_accuracy, avg_pixel_accuracy_fish, avg_precision_fish, avg_recall_fish, avg_f1_fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"DINOv2-FineTuning\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"Architecture\": \"DINOv2-Self-Supervised\",\n",
    "    \"Model\": \"+reg\",\n",
    "    \"Dataset\": \"Fishency\",\n",
    "    \"Batch Size\": 16,\n",
    "    \"Learning_Rate\": 0.0000375,\n",
    "    \"Scheduler\": \"torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\",\n",
    "    \"Epochs\": 300,\n",
    "    \"Optimizer\": \"AdamW(model.parameters(), lr=learning_rate)\",\n",
    "    }\n",
    ")\n",
    "learning_rate = 0.0000375\n",
    "epochs = 300\n",
    "best_iou_for_fish = 0.0\n",
    "best_model_path = 'best_model.pth'\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using \", torch.cuda.device_count(), \"GPUs !!!\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    print(\"Epoch:\", epoch)\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(labeled_train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(pixel_values)\n",
    "\n",
    "        labels = labels.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "            \n",
    "        if torch.cuda.device_count() > 1:\n",
    "            loss = loss.mean() \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "    # Evaluation step\n",
    "    num_classes = 2\n",
    "    avg_eval_loss, avg_mean_iou, iou_per_class, avg_pixel_accuracy, avg_mean_pixel_accuracy, avg_pixel_accuracy_fish, avg_precision_fish, avg_recall_fish, avg_f1_fish = evaluate_model(model, validation_dataloader, device, num_classes)\n",
    "    iou_for_fish = iou_per_class[1]\n",
    "\n",
    "    if iou_for_fish > best_iou_for_fish:\n",
    "        best_iou_for_fish = iou_for_fish\n",
    "        if os.path.exists(best_model_path):\n",
    "            os.remove(best_model_path)\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best IoU for Fish: {iou_for_fish*100:.2f}%. Model saved!\")\n",
    "\n",
    "\n",
    "    if iou_for_fish > 0.80:\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    scheduler.step(avg_eval_loss)\n",
    "    wandb.log({\"learning_rate\": optimizer.param_groups[0]['lr']})\n",
    "\n",
    "    wandb.log({\n",
    "        \"Validation Loss\": avg_eval_loss,\n",
    "        \"Average Mean IoU\": avg_mean_iou,\n",
    "        \"IoU scores for Fish\": iou_for_fish,\n",
    "        \"Pixel Accuracy\": avg_pixel_accuracy,\n",
    "        \"Mean Pixel Accuracy\": avg_mean_pixel_accuracy,\n",
    "        \"Pixel Accuracy for Fish\": avg_pixel_accuracy_fish,\n",
    "        \"Precision for Fish\": avg_precision_fish,\n",
    "        \"Recall for Fish\": avg_recall_fish,\n",
    "        \"F1 Score for Fish\": avg_f1_fish\n",
    "    })\n",
    "\n",
    "    print(f\"Validation Loss: {avg_eval_loss*100:.2f}%\")\n",
    "    print(f\"Average Mean IoU: {avg_mean_iou*100:.2f}%\")\n",
    "    print(f\"IoU scores for fish: {iou_for_fish*100:.2f}%\")\n",
    "    print(f\"Pixel Accuracy: {avg_pixel_accuracy*100:.2f}%\")\n",
    "    print(f\"Mean Pixel Accuracy: {avg_mean_pixel_accuracy*100:.2f}%\")\n",
    "    print(f\"Pixel Accuracy for Fish: {avg_pixel_accuracy_fish*100:.2f}%\")\n",
    "    print(f\"Precision for Fish: {avg_precision_fish*100:.2f}%\")\n",
    "    print(f\"Recall for Fish: {avg_recall_fish*100:.2f}%\")\n",
    "    print(f\"F1 Score for Fish: {avg_f1_fish*100:.2f}%\")\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "random_index = random.randint(0, len(validation_dataset) - 1)\n",
    "sample = validation_dataset[190]\n",
    "sample = video_frames[30]\n",
    "\n",
    "with torch.no_grad():\n",
    "    pixel_values, true_mask, original_image, original_segmentation_map = sample\n",
    "    pixel_values = pixel_values.unsqueeze(0).to(device)  # Add batch dimension and send to device\n",
    "    \n",
    "    outputs = model(pixel_values)\n",
    "    logits = outputs\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    predicted_mask = torch.argmax(probs, dim=1).squeeze().cpu().numpy()  # Remove batch dim\n",
    "\n",
    "# Convert to displayable format\n",
    "original_image_display = np.array(original_image).astype(np.uint8)\n",
    "true_mask_display = np.array(original_segmentation_map).astype(np.uint8)\n",
    "predicted_mask_image = Image.fromarray(predicted_mask.astype(np.uint8))\n",
    "predicted_mask_resized = predicted_mask_image.resize(original_image_display.shape[1::-1], Image.NEAREST)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 10))\n",
    "axes[0].imshow(original_image_display)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(true_mask_display, cmap='jet')\n",
    "axes[1].set_title('True Mask')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(predicted_mask_resized, cmap='jet')\n",
    "axes[2].set_title('Predicted Mask')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define ResizeAndPad class\n",
    "class ResizeAndPad:\n",
    "    def __init__(self, target_size, multiple):\n",
    "        self.target_size = target_size\n",
    "        self.multiple = multiple\n",
    "        \n",
    "    def __call__(self, image):\n",
    "        image = image.resize(self.target_size, Image.BILINEAR)\n",
    "        \n",
    "        # Calculate padding\n",
    "        pad_width = (self.multiple - image.width % self.multiple) % self.multiple\n",
    "        pad_height = (self.multiple - image.height % self.multiple) % self.multiple\n",
    "\n",
    "        # Apply padding\n",
    "        new_width = image.width + pad_width\n",
    "        new_height = image.height + pad_height\n",
    "        new_image = Image.new(\"RGB\", (new_width, new_height))\n",
    "        new_image.paste(image, (pad_width // 2, pad_height // 2))\n",
    "\n",
    "        return new_image\n",
    "\n",
    "# Example values for MEAN and STD; replace with actual values\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "target_size = (448, 448)  # Example target size; adjust as needed\n",
    "\n",
    "# Define the validation transformation pipeline\n",
    "validation_transform_image = transforms.Compose([\n",
    "    ResizeAndPad(target_size, 14),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "def preprocess(image):\n",
    "    try:\n",
    "        image = Image.fromarray(image)\n",
    "        image = validation_transform_image(image)\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocess function: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Paths to input video and output directory\n",
    "input_video_path = \"/home/aleximu/gunes/dinov2/project/videos/output_video_part1.mp4\"\n",
    "output_frames_dir = \"/home/aleximu/gunes/dinov2/outputs_frames_transformed\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_frames_dir, exist_ok=True)\n",
    "\n",
    "# Video capture setup\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Process each frame\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "for frame_idx in tqdm(range(frame_count)):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    try:\n",
    "        pixel_values = preprocess(frame)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing frame: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Convert tensor back to image for saving\n",
    "    pixel_values = pixel_values.permute(1, 2, 0).cpu().numpy()  # Change to HWC format\n",
    "    pixel_values = (pixel_values * np.array(STD) + np.array(MEAN)) * 255.0  # De-normalize\n",
    "    pixel_values = np.clip(pixel_values, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Save the transformed frame as an image\n",
    "    output_frame_path = os.path.join(output_frames_dir, f\"frame_{frame_idx:04d}.png\")\n",
    "    cv2.imwrite(output_frame_path, pixel_values)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "\n",
    "print(f\"Transformed frames saved to {output_frames_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ResizeAndPad:\n",
    "    def __init__(self, target_size, multiple):\n",
    "        self.target_size = target_size\n",
    "        self.multiple = multiple\n",
    "        \n",
    "    def __call__(self, image_tensor):\n",
    "        # Convert tensor to numpy array\n",
    "        image_array = image_tensor.numpy().transpose(1, 2, 0)\n",
    "        image_array = (image_array * 255).astype(np.uint8)  # Ensure it's in the correct range\n",
    "        image = Image.fromarray(image_array)\n",
    "        \n",
    "        # Resize the image\n",
    "        img = transforms.Resize(self.target_size)(image)\n",
    "        \n",
    "        # Calculate padding\n",
    "        pad_width = (self.multiple - img.width % self.multiple) % self.multiple\n",
    "        pad_height = (self.multiple - img.height % self.multiple) % self.multiple\n",
    "\n",
    "        # Apply padding\n",
    "        img = transforms.Pad((pad_width // 2, pad_height // 2, pad_width - pad_width // 2, pad_height - pad_height // 2))(img)\n",
    "        \n",
    "        return img\n",
    "\n",
    "# These are settings for ensuring input images to DinoV2 are properly sized\n",
    "image_dimension = 448\n",
    "    \n",
    "# This is what DinoV2 sees\n",
    "target_size = (image_dimension, image_dimension)\n",
    "\n",
    "# During inference / testing / deployment, we want to remove data augmentations from the input transform:\n",
    "data_transforms = transforms.Compose([ ResizeAndPad(target_size, 14),\n",
    "                                       transforms.CenterCrop(image_dimension),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                                     ]\n",
    "                                     )\n",
    "\n",
    "image_size = (image_dimension, image_dimension)\n",
    "output_dir = '.'\n",
    "patch_size = 14\n",
    "n_register_tokens = 4\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "# Get the sample from the dataset\n",
    "sample = validation_dataset[6]\n",
    "original_image = sample[0]  # Assuming the first element is the image\n",
    "\n",
    "original_h, original_w = original_image.shape[1], original_image.shape[2]\n",
    "\n",
    "original_h = int(original_h)\n",
    "original_w = int(original_w)\n",
    "\n",
    "# Apply the data transformations (now compatible with tensor input)\n",
    "img = data_transforms(original_image)\n",
    "\n",
    "# Make the image divisible by the patch size\n",
    "w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - img.shape[2] % patch_size\n",
    "img = img[:, :w, :h]\n",
    "\n",
    "w_featmap = img.shape[-2] // patch_size\n",
    "h_featmap = img.shape[-1] // patch_size\n",
    "\n",
    "# Prepare the image for the model\n",
    "img = img.unsqueeze(0)\n",
    "img = img.to(device)\n",
    "\n",
    "attention = model.module.get_last_self_attention(img.to(device))\n",
    "\n",
    "number_of_heads = attention.shape[1]\n",
    "\n",
    "# attention tokens are packed in after the first token; the spatial tokens follow\n",
    "attention = attention[0, :, 0, 1 + n_register_tokens:].reshape(number_of_heads, -1)\n",
    "\n",
    "# resolution of attention from transformer tokens\n",
    "attention = attention.reshape(number_of_heads, w_featmap, h_featmap)\n",
    "\n",
    "# upscale to higher resolution closer to original image\n",
    "attention = nn.functional.interpolate(attention.unsqueeze(0), scale_factor=patch_size, mode = \"nearest\")[0].cpu()\n",
    "\n",
    "# sum all attention across the 12 different heads, to get one map of attention across entire image\n",
    "attention = torch.sum(attention, dim=0)\n",
    "\n",
    "# interpolate attention map back into original image dimensions\n",
    "attention_of_image = nn.functional.interpolate(attention.unsqueeze(0).unsqueeze(0), size=(original_h, original_w), mode='bilinear', align_corners=False)\n",
    "attention_of_image = attention_of_image.squeeze()\n",
    "\n",
    "# Normalize image_metric to the range [0, 1]\n",
    "image_metric = attention_of_image.numpy()\n",
    "normalized_metric = Normalize(vmin=image_metric.min(), vmax=image_metric.max())(image_metric)\n",
    "\n",
    "# Apply the Reds colormap\n",
    "reds = plt.cm.Reds(normalized_metric)\n",
    "\n",
    "# Create the alpha channel\n",
    "alpha_max_value = 1.00  # Set your max alpha value\n",
    "\n",
    "# Adjust this value as needed to enhance lower values visibility\n",
    "gamma = 0.5  \n",
    "\n",
    "# Apply gamma transformation to enhance lower values\n",
    "enhanced_metric = np.power(normalized_metric, gamma)\n",
    "\n",
    "# Create the alpha channel with enhanced visibility for lower values\n",
    "alpha_channel = enhanced_metric * alpha_max_value\n",
    "\n",
    "# Add the alpha channel to the RGB data\n",
    "rgba_mask = np.zeros((image_metric.shape[0], image_metric.shape[1], 4))\n",
    "rgba_mask[..., :3] = reds[..., :3]  # RGB\n",
    "rgba_mask[..., 3] = alpha_channel  # Alpha\n",
    "\n",
    "# Convert the numpy array to PIL Image\n",
    "rgba_image = Image.fromarray((rgba_mask * 255).astype(np.uint8))\n",
    "\n",
    "# Save the image\n",
    "rgba_image.save('attention_mask.png')\n",
    "\n",
    "# Assuming 'validation_dataset' is your dataset and it returns a tuple (image, label)\n",
    "\n",
    "# Get the sample from the dataset\n",
    "sample = validation_dataset[6]\n",
    "original_image_tensor = sample[0]  # Assuming the first element is the image\n",
    "\n",
    "# Convert tensor to numpy array\n",
    "original_image_np = original_image_tensor.numpy().transpose(1, 2, 0)\n",
    "\n",
    "# Convert numpy array to PIL image\n",
    "original_image = Image.fromarray((original_image_np * 255).astype(np.uint8))\n",
    "\n",
    "# Load the attention mask with PIL\n",
    "attention_mask_image = Image.open(\"{}/attention_mask.png\".format(output_dir))\n",
    "\n",
    "# Ensure both images are in the same mode\n",
    "if original_image.mode != 'RGBA':\n",
    "    original_image = original_image.convert('RGBA')\n",
    "if attention_mask_image.mode != 'RGBA':\n",
    "    attention_mask_image = attention_mask_image.convert('RGBA')\n",
    "\n",
    "# Resize the attention mask to match the original image dimensions if necessary\n",
    "attention_mask_image = attention_mask_image.resize(original_image.size, Image.ANTIALIAS)\n",
    "\n",
    "# Overlay the second image onto the first image\n",
    "original_image.paste(attention_mask_image, (0, 0), attention_mask_image)\n",
    "\n",
    "# Save or show the combined image\n",
    "original_image.save('image_with_attention.png')\n",
    "\n",
    "# Or display it\n",
    "display(original_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
